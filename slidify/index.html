<!DOCTYPE html>
<html>
<head>
  <title></title>
  <meta charset="utf-8">
  <meta name="description" content="">
  <meta name="author" content="">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1></h1>
    <h2></h2>
    <p><br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="class" id="introduction" style="background:white;">
  <hgroup>
    <h2>Introduction</h2>
  </hgroup>
  <article data-timings="">
    <p>This project aims at providing a usable way of predicting the next word a user would most likely want to type, given an input of a string of text. This presentation explains the techniques used, but the code is available as well.</p>

<p>Topics:</p>

<ul>
<li>Processing data</li>
<li>Prediction algorythm</li>
<li>Testing the algorythm</li>
<li>Shiny App</li>
</ul>

<p>Links:</p>

<ul>
<li>Github: <a href="https://github.com/sjakil/datascience-capstone">https://github.com/sjakil/datascience-capstone</a></li>
</ul>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="data" style="background:white;">
  <hgroup>
    <h2>Processing data</h2>
  </hgroup>
  <article data-timings="">
    <p>One of the first choices to make was which R libraries to use for text mining. I have explored two options, <code>tm</code> and <code>quanteda</code>. After testing, it seems <code>quanteda</code> is alot faster and easier to use.</p>

<p>Text mining starts by getting a corpus of text and cleaning it. The available training data consists of around 600MB of lines originating from twitter, blogs posts and news articles. I created a process capable of handling this and which is very easy to use. </p>

<p>This process loads a random sample of lines, cleans them and creates a corpus of text. I determined the following steps in cleaning yielded the most promising results:</p>

<ul>
<li>Remove time notations (xx pm/am) and unit notations (xx kg/mm/etc)</li>
<li>Remove special encoding (Ã¤ &gt; a), Twitter (RT, @ etc), hyphens, numbers and punctuation.</li>
</ul>

<pre><code>## [1] &quot;Loading file: ../final/en_US/en_US.twitter.txt ...&quot;
## [1] &quot;Sample size loaded: 10000&quot;
</code></pre>

<pre><code>## [1] &quot;Creating corpus ...&quot;
</code></pre>

<pre><code>## Error in createCorpus(lines[((x - 1) * chunkSize):(x * chunkSize)]): could not find function &quot;toLower&quot;
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="prediction" style="background:white;">
  <hgroup>
    <h2>Prediction algorythm</h2>
  </hgroup>
  <article data-timings="">
    <p>After getting the corpus of text we can start creating n-grams. In case you are not familiar with them, n-grams are combinations of n amount of words. For example, a common bigram (2-gram) is &quot;you are&quot;. In this project, I&#39;ve created a maximum of 5 grams.</p>

<p>The next step is taking every n-gram except the unigram (1-gram) and splitting off the last word (let&#39;s call it &quot;tail&quot;) from the words that came before (&quot;head&quot;). Some of these heads will appear more than once and have different tails. By simply counting the frequency of tails and heads, we can calculate probabilities.</p>

<p>The final step is to produce a table that contains these probabilities. Every head of n grams is essentially a n - 1 gram as well. This makes it so that the head of a quintgram (5-gram) could be matched to a quadgram (4-gram) input. So  a model with n-grams up to 5 n&#39;s, would be able to make predictions based on input of up to 4 grams.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="testing" style="background:white;">
  <hgroup>
    <h2>Testing the algorythm</h2>
  </hgroup>
  <article data-timings="">
    <p>In order to test the algorythm we are going to repeat the process described before, but this time we will split the lines of text into two parts. The one part is used to train the model (80% of data), the other part is used to test the model (20% of data). After creating n-grams and probabilities for both sets, we can calculate how many our model predicts correctly.</p>

<p>I&#39;ve created a function that will make doing these tests very easy. In the example below I&#39;ve used 50k lines of all three sources for up to quintgrams.</p>

<pre><code class="r">accuracyTest(files, maxGram = 5, verboseLevel = 1)
</code></pre>

<pre><code>## [1] &quot;Creating probabilities ...&quot;
## [1] &quot;2-Gram accuracy: 7.68&quot;
## [1] &quot;3-Gram accuracy: 27.48&quot;
## [1] &quot;4-Gram accuracy: 46.78&quot;
## [1] &quot;5-Gram accuracy: 54.74&quot;
## [1] &quot;Total accuracy: 18.72&quot;
</code></pre>

  </article>
  <!-- Presenter Notes -->
</slide>

<slide class="class" id="app" style="background:white;">
  <hgroup>
    <h2>Shiny App</h2>
  </hgroup>
  <article data-timings="">
    <p>The Shiny App provides a straight-forward way of using the prediction algorythm. You enter a string of text and the app almost instantly gives you the prediction it found.</p>

<p>It also implements what is called stupid backoff. This means if the app could not predict a word given an n-gram as input, it will try to predict the next word using the n - 1 gram as input. You can toggle this on or off in the side panel.</p>

<p>To make the probabilities works really fast, I&#39;ve used the <code>data.table</code> package and it&#39;s <code>setkey</code> function. It creates an index on a specified column and this presumably works up to a 1000x times faster compared to normal data frames.</p>

  </article>
  <!-- Presenter Notes -->
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Introduction'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Processing data'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Prediction algorythm'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Testing the algorythm'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Shiny App'>
         5
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>